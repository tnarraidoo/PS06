---
title: "STAT/MATH 495: Problem Set 06"
author: "Tasheena Narraidoo"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE, options(scipen=999)
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
library(Metrics)


```





# Collaboration

Please indicate who you collaborated with on this assignment: -





# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sample <- 10000
```


# Computation

$$ MSE = \mathbb{E}[(Y - \hat{f}(x))^2] = mean(\ (y - estimate)^2\ )  $$
$$Bias^2 = (\ \mathbb{E}[\hat{f}(x)] -f(x)\ )^2 = (\ mean(estimate) - f(x)\ )^2$$

$$Variance = \mathbb{E} [ (\ \hat{f}(x) - \mathbb{E}[\hat{f}(x)]\ )^2] = mean(\ (estimate - mean(estimate)\ )^2\ ) $$
$$ Irreducible = \sigma^2_{\epsilon}  $$


$$ Sum = Bias^2 + Variance + Irreducible $$
```{r}
# mse function
eval_err <- function(trueY, estimate){
  return (mean((trueY-estimate)^2))
}

# bias function
eval_bias_sq <- function(actual, estimate){
  return ((mean(estimate) - actual)^2)
}

# variance function
eval_var <- function(estimate){
  return (mean((estimate - mean(estimate))^2))
}
```

```{r}
set.seed(495)

# set some values
f_0.95 <- c(0.95^2) # f(x0) where x0=.95
# calculate irreducible error
irr_err <- sigma^2

# vectors
predictions_df_2 <- runif(n=n_sample) # vector to contain df=2 predictions
predictions_df_99 <- runif(n=n_sample) # vector to contain df=99 predictions
actual_val <- runif(n=n_sample) # vector to contain true y

for(i in 1:n_sample){
  sampled_points <- generate_sample(f, n, sigma)
  # calculate y 
  actual_val[i] <- 0.95^2 + rnorm(1, mean = 0, sd = sigma)
  
  # fit model for df = 2
  fitted_df_2 <- smooth.spline(x=sampled_points$x, y=sampled_points$y, df=2) 
  # make prediction for x0 = 0.95
  pred_df_2 <- predict(fitted_df_2, c(0.95))[2] %>% unlist()
  predictions_df_2[i] <- pred_df_2
  
  # fit model for df = 99
  fitted_df_99 <- smooth.spline(x=sampled_points$x, y=sampled_points$y, df=99) 
  # make prediction for x0 = 0.95
  pred_df_99 <- predict(fitted_df_99, c(0.95))[2] %>% unlist()
  predictions_df_99[i] <- pred_df_99
  
}


# for df = 2:
#   calculate mse
err_df_2 <- eval_err(trueY = actual_val, estimate = predictions_df_2)
#   calculate bias^2
bias_sq_df_2 <- eval_bias_sq(actual = f_0.95, estimate = predictions_df_2)
#   calculate variance 
var_df_2 <- var(predictions_df_2) 
#   calculate sum for df = 2
sum_df_2 <- var_df_2 + bias_sq_df_2 + irr_err

# for df = 99:
#   calculate mse
err_df_99 <- eval_err(trueY = actual_val, estimate = predictions_df_99)
#   calculate bias^2
bias_sq_df_99 <- eval_bias_sq(actual = f_0.95, estimate = predictions_df_99)
#   calculate variance 
var_df_99 <- var(predictions_df_99) 
#   calculate sum for df = 99
sum_df_99 <- var_df_99 + bias_sq_df_99 + irr_err
```


# Tables

As done in Lec 2.7, for both

* An `lm` regression AKA a `smooth.splines(x, y, df=2)` model fit 
* A `smooth.splines(x, y, df=99)` model fit 

output tables comparing:

|  MSE| bias_squared|   var| irreducible|   sum|
|----:|------------:|-----:|-----------:|-----:|
|     X|           X  |     X |      X |         X |

where `sum = bias_squared + var + irreducible`. You can create cleanly formatted tables like the one above by piping a data frame into `knitr::kable(digits=4)`.

```{r, echo=FALSE}
# create table with results
df <- c("df = 2", "df = 99")
mse <- c(round(err_df_2,4), round(err_df_99,4))
bias_squared <- c(round(bias_sq_df_2,4), round(bias_sq_df_99,4))
var <- c(round(var_df_2,4), round(var_df_99,4))
irreducible <- c(round(irr_err,4), round(irr_err,4))
sum <- c(round(sum_df_2,4), round(sum_df_99,4))

tbl <- cbind(df, 
             mse, 
             bias_squared,
             var,
             irreducible,
             sum
             )
colnames(tbl) <- c(" ", "MSE", "bias_squared", "variance", "irreducible", "sum")
tbl %>% knitr::kable(digits=4)
```

# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
1. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
1. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. I would compare the differences of the bias squared and the variance values of both models. For our df=2 model, we would expect the bias squared value to be higher than that of our df=3 model. This is because the model would be a straight line and would therefore not give predictions that are closer to the true value as compared to our df=99 model. On the other hand, because of the straight line nature of our df=2 model, we would expect it to have a lower variance than that of our df=99 model because of the variance-bias tradeoff. Our df=99 model would give us more accuracy predictions however, we will have higher variability. So, a sanity check would be to compare bias squared and the variance values of both models. A contradictory pattern would be a red flag. 

  Another sanity check would be to look at the true value of our target point and the predicted values of the target point. For our df=99 model, I would expect the true value of our target point to be in our prediction interval while I would not necessarily expect the true value of our target point to be in our prediction interval for our df=2 model.So, if the true value is not within the prediction interval for the df=99 model, I would suspect there is something wrong with our calculation. The difference between sum and mse should be relatively small as though obtained differently, sum and mse represent the same error.

2. We follow the same procedure but when we predict our model, we use the entire domain if our function f(x) as our test set and not a single target point. We will have vectors that will store each type of error for each point in our target vector point.

[

A sketch of the procedure would be as follows:

Suppose we will run n_sample = 10,000 simulations with n = 500.

First, we would generate random x values along the lines of:

x <- runif(n = n_sample, min = 0, max =1)

Then we will generate y values, based on the x values.

y <- rnorm(n = n_sample, mean = f(x), sd = sigma)

From there we can fit our models, get $\hat{y}$ through the predict() function and apply the eval_err, eval_bias_sq, eval_var methods for each estimate of our domain x. Here we will have a vector storing those values for each estimate. The irreducible error will stay the same.

]

3. If there is a considerable difference between the error of both models, I would choose the model with the lower error. However, if the difference in error is so small, I think it would depend on what my goal is. If my goal is solely to get an accurate a prediction of the point of interest, I would choose the df=99 model. However, if I am concerned with variability, I would choose the df=2 model. I would also consider which is a bigger 'danger' - over-fitting or under-fitting. If over-fitting is better than under-fitting, I would go for the model with higher complexity and vice versa.

# References

* Understanding the Bias-Variance Tradeoff ([link](http://scott.fortmann-roe.com/docs/BiasVariance.html)) Lecture 2.7

